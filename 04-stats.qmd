---
title: "Basic Inferential Stats in R: Correlation, T-Tests, and ANOVA"
author: "Bella Ratmelia"
format: revealjs
---

## Today's Outline

1.  Refreshers on data distribution and research variables
2.  Statistical tests: chi-square, t-test, correlations.
3.  ANOVA

## Refresher: Data Distribution

The choice of appropriate statistical tests and methods often depends on the distribution of the data. Understanding the distribution helps in selecting the right and validity of the tests.

```{r}
# Set a seed for reproducibility
set.seed(123)

normal_data <- rnorm(1000, mean = 0, sd = 1)
left_skewed_data <- rbeta(1000,5,2)
right_skewed_data <- rbeta(1000,2,5)
bimodal_data <- c(rnorm(500, mean = -2, sd = 1), rnorm(500, mean = 2, sd = 1))
uniform_data <- runif(1000, min = -1, max = 1)

par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

# Plot histograms or density plots for each distribution
hist(normal_data, main = "Normal Distribution", col = "lightblue", probability = TRUE)
curve(dnorm(x, mean = mean(normal_data), sd = sd(normal_data)), add = TRUE, col = "blue", lwd = 2)

hist(bimodal_data, main = "Bimodal Distribution", col = "lightgoldenrod", probability = TRUE)
lines(density(bimodal_data), col = "darkorange", lwd = 2)

hist(uniform_data, main = "Uniform Distribution", col = "lightpink", probability = TRUE)
curve(dunif(x, min = -1, max = 1), add = TRUE, col = "deeppink", lwd = 2)

hist(left_skewed_data, main = "Left Skewed Distribution", col = "lightgreen", probability = TRUE)
curve(dbeta(x, 5, 2), add = TRUE, col = "darkgreen", lwd = 2)

hist(right_skewed_data, main = "Right Skewed Distribution", col = "lightcoral", probability = TRUE)
curve(dbeta(x, 2, 5), add = TRUE, col = "darkred", lwd = 2)
```

## Refresher: Research Variables

::: columns
::: {.column width="50%"}
[**Independent Variable (IV)**]{.underline}

The variables that researchers will manipulate.

-   Other names for it: Predictor, Covariate, Treatment, Regressor, Input, etc.
:::

::: {.column width="50%"}
[**Dependent Variable (DV)**]{.underline}

The variables that will be affected as a result of manipulation/changes in the IVs

-   Other names for it: Outcome, Response, Output, etc.
:::
:::

## Load our data for today!

Use `read_csv` from `readr` package (part of `tidyverse`) to load our data into a dataframe

```{r}
#| echo: true
#| label: load-data
#| message: false
#| output: false

# import tidyverse library
library(tidyverse)

# read the CSV with Chile voting data
chile_data <- read_csv("data/chile_voting.csv")

# peek at the data, pay attention to the data types!
glimpse(chile_data)
```

## Chi-square test of independence

The $X^2$ test of independence evaluates whether there is a statistically significant relationship between two categorical variables.

This is done by analyzing the frequency table (i.e., contingency table) formed by two categorical variables.

**Example**: Is there any relationship between `education` and `vote` in our Chile voting data?

## Chi-Square: Sample problem and results

Is there any relationship between `education` and `vote` in our Chile voting data?

```{r}
#| echo: true
print(chisq.test(table(chile_data$education, chile_data$vote)))
```

::: {style="font-size: 75%; width: 75%"}
-   **X-squared** = the coefficient
-   **df** = degree of freedom
-   **p-value** = the probability of getting more extreme results than what was observed. Generally, if this value is less than the pre-determined significance level (also called alpha), the result would be considered "statistically significant"
:::

*What if there is a hypothesis? How would you write this in the report?*

## Correlation

A correlation test evaluates the strength and direction of a linear relationship between two variables. The coefficient is expressed in value between -1 to 1, with 0 being no correlation at all.

::: columns
::: {.column width="40%"}
**Pearson's** $r$ (r)

-   Assumes and measures linear relationships
-   Sensitive to outliers
-   Assumes normality
-   (*most likely the one that you learned in class)*
:::

::: {.column width="30%"}
**Kendall's** $\tau$ (tau)

-   Measures monotonic relationship
-   less sensitive/more robust to outliers
-   non-parametric, do not assume normality
:::

::: {.column width="30%"}
**Spearman's** $\rho$ (rho)

-   Measures monotonic relationship
-   less sensitive/more robust to outliers
-   non-parametric, do not assume normality
:::
:::

## What is monotonic relationship?

When a size of one variable increases, the other variables also increase, though at different rate.

```{r}

par(mfrow = c(1, 2))

# Monotonic Relationship 
set.seed(123)
x_monotonic <- seq(1, 20, length.out = 25)
y_monotonic <- c(1, 2, 3, 4, 5, 
                 5, 7, 8, 8, 10, 
                 10, 10, 10, 10, 10 ,
                10 ,10, 10, 11, 11, 
                 11, 12, 14, 15, 16)

plot(x_monotonic, y_monotonic, main = "Monotonic Relationship", 
     xlab = "X", ylab = "Y", pch = 3, col = "navy")

# Linear Relationship 
set.seed(456)
x_linear <- seq(1, 20, length.out = 25)
y_linear <- x_linear + rnorm(25, mean = 0, sd = 1)

plot(x_linear, y_linear, main = "Linear Relationship", 
     xlab = "X", ylab = "Y", pch = 4, col = "purple")

par(mfrow = c(1, 1))

```

## Correlation: Sample problem and result

Is there a significant correlation between a respondent's age and income?

```{r}
#| echo: true
cor.test(chile_data$age, chile_data$income, method = "pearson")
```

::: {style="font-size: 75%; width: 75%"}
-   **t** is the **t-test statistic** (for hypothesis testing)
-   **df** is the degrees of freedom
-   **p-value** is the significance level of the **t-test**
-   **conf.int** is the **confidence interval** of the coefficient at 95%
-   **sample estimates** is the correlation coefficient
:::

## Let's try this correlation exercise! (5 mins)

-   Calculate the correlation coefficient between age and statusquo.

    -   What is the null and alternative hypothesis?

    -   How strong is the correlation? i.e. would you say it's a strong correlation?

    -   In which direction is the correlation?

    -   Is the correlation coefficient statistically significant?

```{r}
#| echo: false
#| output: false
cor.test(chile_data$age, chile_data$statusquo, method = "pearson")

```

## T-Tests

A **t-test** is a statistical test used to compare the means of two groups/samples of continuous data type and determine if the differences are statistically significant.

-   The Student's *t*-test is widely used when the sample size is reasonably small (less than approximately 30) **or** when the population standard deviation is unknown.

## 3 types of t-test

::: columns
::: {.column width="30%"}
**One-sample T-test**

Test if a specific sample mean (X̄) is statistically different from a known or hypothesized population mean (μ or mu)
:::

::: {.column width="40%"}
**Two-samples / Independent Samples T-test**

Used to compare the means of two independent groups (such as between-subjects research) to determine if they are significantly different.

Examples: Men vs Women group, Placebo vs Actual drugs.
:::

::: {.column width="30%"}
**Paired Samples T-Test**

Used to compare the means of two related groups, such as repeated measurements on the same subjects (within-subjects research).

Examples: Before workshop vs After workshop.
:::
:::

## T-test: Sample problem and result

Is the average age of voters who intend to vote "Yes" significantly different from the overall mean age of voters in Chile? Assume the

```{r}
#| echo: true
overall_mean_age = mean(chile_data$age, na.rm = TRUE)
yes_voters = chile_data$age[chile_data$vote == "Y"]
t.test(x = yes_voters, alternative = "two.sided", mu = overall_mean_age)
```

*What if there is a hypothesis? How would you write this in the report?*

## Let's try this T-Test exercises (5 mins)

1.  Is there a significant difference in age between those who intend to vote "Yes" and those who intend to vote "No"?
2.  Does the income of male voters significantly differ from female voters?

```{r}
#| echo: false
#| output: false
t.test(age ~ vote, data = chile_data[chile_data$vote %in% c("Y", "N"),])
```

```{r}
#| echo: false
#| output: false
t.test(income ~ sex, data = chile_data)
```

## ANOVA (Analysis of Variance)

ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups or samples and determine if the differences are statistically significant.

There are two 'mainstream' ANOVA that will be covered in this workshop:

::: incremental
-   **One-Way ANOVA**: comparing means across two or more independent groups (levels) of a [**single**]{.underline} independent variable.
-   **Two-Way ANOVA**: comparing means across two or more independent groups (levels) of [**two**]{.underline} independent variable.
-   Other types of ANOVA that you may encounter: Repeated measures ANOVA, Multivariate ANOVA (MANOVA), ANCOVA, etc.
:::

## One-Way ANOVA: Sample problem and result

Is there a significant difference in statusquo scores between different education levels?

```{r}
#| echo: true
statusquo_edu_anova <- aov(statusquo ~ education, data = chile_data)
summary(statusquo_edu_anova)
```

::: {style="font-size: 75%; width: 75%"}
-   **F-value**: the coefficient value
-   **Pr(\>F)**: the p-value
-   **Sum Sq**: Sum of Squares
-   **Mean Sq** : Mean Squares
-   **Df**: Degrees of Freedom
:::

## Post-hoc test (only when result is significant)

If your ANOVA test indicates significant result, the next step is to figure out which category pairings are yielding the significant result. Tukey's Honest Significant Difference (HSD) can help us figure that out.

```{r}
#| echo: true
TukeyHSD(statusquo_edu_anova)
```

## Two-Way ANOVA: Sample problem and result

Is there a significant difference in statusquo scores between different education levels and sexes?

```{r}
#| echo: true
#| output: true
statusquo_edu_sex_anova <- aov(statusquo ~ education * sex, data = chile_data)
summary(statusquo_edu_sex_anova)
```

## Post-hoc test for Two-way ANOVA

```{r}
#| echo: true
TukeyHSD(statusquo_edu_sex_anova)
```

## Let's try this ANOVA exercise! (5 mins)

Is there a significant difference in income between different regions and education levels?

```{r}
#| echo: false
#| output: false
income_region_edu_anova <- aov(income ~ region * education, data = chile_data)
summary(income_region_edu_anova)
TukeyHSD(income_region_edu_anova)
```

## ANOVA Assumptions

1.  The Dependent variable should be a continuous variable

2.  The Independent variable should be a categorical variable

3.  The observations for Independent variable should be independent of each other

4.  The Dependent Variable distribution should be approximately normal -- even more crucial if sample size is small.

    -   You can verify this by visualizing your data in histogram, or use Shapiro-Wilk Test, among other things

5.  The variance for each combination of groups should be approximately equal -- also referred to as "homogeneity of variances" or [**homoskedasticity**]{.underline}.

    -   One way to verify this is using Levene's Test

6.  No significant outliers

## Verifying the assumptions

**Shapiro-Wilk Test**

```{r}
#| echo: true

library(car)
shapiro.test(chile_data$income)
```

**Levene's Test**

```{r}
#| echo: true

library(car)
leveneTest(income ~ education, data = chile_data)
```

# End of Session 4!

Next session: Linear and Logistic Regressions!
