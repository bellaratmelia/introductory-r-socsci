---
title: "Basic Inferential Stats in R: Correlation, T-Tests, and ANOVA"
author: "Bella Ratmelia"
format: revealjs
---

## Today's Outline

1.  Refreshers on data distribution and research variables
2.  Statistical tests: chi-square, t-test, correlations.
3.  ANOVA

## Refresher: Data Distribution

The choice of appropriate statistical tests and methods often depends on the distribution of the data. Understanding the distribution helps in selecting the right and validity of the tests.

```{r}
# Set a seed for reproducibility
set.seed(123)

normal_data <- rnorm(1000, mean = 0, sd = 1)
left_skewed_data <- rbeta(1000,5,2)
right_skewed_data <- rbeta(1000,2,5)
bimodal_data <- c(rnorm(500, mean = -2, sd = 1), rnorm(500, mean = 2, sd = 1))
uniform_data <- runif(1000, min = -1, max = 1)

par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

# Plot histograms or density plots for each distribution
hist(normal_data, main = "Normal Distribution", col = "lightblue", probability = TRUE)
curve(dnorm(x, mean = mean(normal_data), sd = sd(normal_data)), add = TRUE, col = "blue", lwd = 2)

hist(bimodal_data, main = "Bimodal Distribution", col = "lightgoldenrod", probability = TRUE)
lines(density(bimodal_data), col = "darkorange", lwd = 2)

hist(uniform_data, main = "Uniform Distribution", col = "lightpink", probability = TRUE)
curve(dunif(x, min = -1, max = 1), add = TRUE, col = "deeppink", lwd = 2)

hist(left_skewed_data, main = "Left Skewed Distribution", col = "lightgreen", probability = TRUE)
curve(dbeta(x, 5, 2), add = TRUE, col = "darkgreen", lwd = 2)

hist(right_skewed_data, main = "Right Skewed Distribution", col = "lightcoral", probability = TRUE)
curve(dbeta(x, 2, 5), add = TRUE, col = "darkred", lwd = 2)
```

## Refresher: Research Variables

::: columns
::: {.column width="50%"}
[**Dependent Variable (DV)**]{.underline}

The variables that will be affected as a result of manipulation/changes in the IVs

-   Other names for it: Outcome, Response, Output, etc.
-   Often denoted as $y$
:::

::: {.column width="50%"}
[**Independent Variable (IV)**]{.underline}

The variables that researchers will manipulate.

-   Other names for it: Predictor, Covariate, Treatment, Regressor, Input, etc.
-   Often denoted as $x$
:::
:::

## Open your project - the (possibly) easier way

1.  Go to the folder where you put your project for this workshop

2.  Find a file with `.Rproj` extension - this is the R project file that holds all the information about your project.

![](images/locate-rproj-file.jpg)

3.  Double click on the file. Rstudio should launch with your project loaded! This should be easier to ensure that you are loading the correct project when opening Rstudio.


## Load our data for today!

Let's create a new R script called `session-3.R`, and then copy the code below to load our data for today. This code uses `read_csv` from `readr` package (part of `tidyverse`) to load our cleaned CSV (from the first checkpoint)

```{r}
#| echo: true
#| label: load-data
#| message: false
#| output: false

# import tidyverse library
library(tidyverse)

# read the CSV with WVS data
wvs_cleaned <- read_csv("data-output/wvs_cleaned_v1.csv")

# Convert categorical variables to factors
columns_to_convert <- c("country", "religiousity", "sex", "marital_status", "employment")

wvs_cleaned <- wvs_cleaned |> 
    mutate(across(all_of(columns_to_convert), as_factor))

# peek at the data, pay attention to the data types!
glimpse(wvs_cleaned)
```
# Both Categorical variables - The $X^2$ test

## Chi-square test of independence

The $X^2$ test of independence evaluates whether there is a statistically significant relationship between two categorical variables.

This is done by analyzing the frequency table (i.e., contingency table) formed by two categorical variables.

**Example**: Is there a relationship between `religiousity` and `country` in our WVS data?

Typically, we can start with the contigency table first, and then the visualization

```{r}
#| echo: true
table(wvs_cleaned$religiousity, wvs_cleaned$country)
```


## Chi-square test of independence - visualizing data

We can use barchart to visualize this (remember from last week!)

```{r}
#| echo: true

wvs_cleaned |> 
    ggplot(aes(x = country, fill = religiousity)) +
    geom_bar(position = "fill") +
    labs(title = "Proportion of religiousity for each country") +
    theme_minimal()

```

## Chi-Square: Sample problem and results

Is there a relationship between religiosity and country?

```{r}
#| echo: true
chisq.test(table(wvs_cleaned$religiousity, wvs_cleaned$country))
```

::: {style="font-size: 75%; width: 75%"}
-   **X-squared** = the coefficient
-   **df** = degree of freedom
-   **p-value** = the probability of getting more extreme results than what was observed. Generally, if this value is less than the pre-determined significance level (also called alpha), the result would be considered "statistically significant"
:::

*What if there is a hypothesis? How would you write this in the report?*

# Both Continuous variables - Correlation

## Correlation

A correlation test evaluates the strength and direction of a linear relationship between two variables. The coefficient is expressed in value between -1 to 1, with 0 being no correlation at all.

::: columns
::: {.column width="33%"}
**Pearson's** $r$ (r)

-   Measure the association between two continuous numerical variables
-   Sensitive to outliers
-   Assumes normality and/or linearity
-   (*most likely the one that you learned in class)*
:::

::: {.column width="33%"}
**Kendall's** $\tau$ (tau)

-   Measure the association between two variables (ordinal-ordinal or ordinal-continuous)
-   less sensitive/more robust to outliers
-   non-parametric, does not assume normality and/or linearity
:::

::: {.column width="33%"}
**Spearman's** $\rho$ (rho)

-   Measure the association between two variables (ordinal-ordinal or ordinal-continuous)
-   less sensitive/more robust to outliers
-   non-parametric, does not assume normality and/or linearity
:::
:::

For more info, you can refer to this reading: [Measures of Association - How to Choose? (Harry Khamis, PhD)](https://journals.sagepub.com/doi/pdf/10.1177/8756479308317006){target="_blank"}

## Correlation: Sample problem and result

RQ: **Is there a significant correlation between life satisfaction and financial satisfaction?**

As both variables are numerical and continuous, we can use pearson correlation.

Let's start with visualizing the data, which can be used to support the explanation.

```{r}
#| echo: true
#| output-location: slide
#| 
wvs_cleaned |> 
    ggplot(aes(x = financial_satisfaction, y = life_satisfaction)) +
    geom_jitter(color="maroon", alpha=0.5) +
    geom_smooth(method = "lm", se = TRUE) # se shows the confidence interval
```

## Conduct the correlation test

```{r}
#| echo: true
#| output-location: column
cor.test(wvs_cleaned$life_satisfaction, 
         wvs_cleaned$financial_satisfaction, 
         method = "pearson")
```

::: {style="font-size: 75%; width: 75%"}
-   **cor** is the **correlation coefficient** - this is the number that you want to report.
-   **t** is the **t-test statistic**
-   **df** is the degrees of freedom
-   **p-value** is the significance level of the **t-test**
-   **conf.int** is the **confidence interval** of the coefficient at 95%
-   **sample estimates** is the correlation coefficient
:::

## Learning Check #1

Is there a relationship between religiousity and age group? Calculate the chi-square test!

optional: Visualize the proportion of age group for each religiousity category.


```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show answer"
chisq.test(table(wvs_cleaned$religiousity, wvs_cleaned$age_group))

```
**Visualization:**

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show answer"
#| output-location: slide

wvs_cleaned |> 
    ggplot(aes(x = religiousity, fill = age_group)) +
    geom_bar(position = "fill") +
    labs(title = "Proportion of age groups for each religiousity") +
    theme_minimal()

```
# Comparing Means Between Groups - T-Tests and ANOVA

## T-Tests

A **t-test** is a statistical test used to compare the means of two groups/samples of continuous data type and determine if the differences are statistically significant.

-   The Student's *t*-test is widely used when the sample size is reasonably small (less than approximately 30) **or** when the population standard deviation is unknown.

## 3 types of t-test

::: columns
::: {.column width="30%"}
**One-sample T-test**

Test if a specific sample mean (X̄) is statistically different from a known or hypothesized population mean (μ or mu)
:::

::: {.column width="40%"}
**Two-samples / Independent Samples T-test**

Used to compare the means of two independent groups (such as between-subjects research) to determine if they are significantly different.

Examples: Men vs Women group, Placebo vs Actual drugs.
:::

::: {.column width="30%"}
**Paired Samples T-Test**

Used to compare the means of two related groups, such as repeated measurements on the same subjects (within-subjects research).

Examples: Before workshop vs After workshop.
:::
:::

## T-test: One-sample T-Test

RQ: **Is the average life satisfaction in our sample significantly different from the global average of 6.5?**

Let's start with visualizing the data

```{r}
#| echo: true
#| output-location: fragment

wvs_cleaned |> 
    ggplot(aes(y = life_satisfaction)) +
    geom_boxplot(width = 0.2) + 
    geom_hline(yintercept = 6.5, color="red") 
```

## Conduct the One-sample T-Test

```{r}
#| echo: true
#| output-location: fragment
global_mean_satisfaction = 6.5  

t.test(wvs_cleaned$life_satisfaction,          
       alternative = "two.sided", 
       mu = global_mean_satisfaction)

```

*What if there is a hypothesis? How would you write this in the report?*

## T-Test: Independent Samples T-Test

RQ: **Is there a significant difference in life satisfaction between males and females?**

Let's first take only the necessary columns and get some summary statistics, particularly on the number of samples for each group, as well as the mean, standard deviation, and variance.

```{r}
#| echo: true
#| output-location: fragment

wvs_cleaned |> 
    group_by(sex) |> 
    summarise(total = n(), 
              mean = mean(life_satisfaction),
              variance = var(life_satisfaction),
              stdeviation = sd(life_satisfaction))
```

## Visualize the differences between two samples

The variance will be easier to see when we visualize it as well. As we can see, the variance for both groups are about the same. This suggests that the variance might be homoegeneous.

```{r}
#| echo: true
#| output-location: fragment
#| 
wvs_cleaned |> 
    ggplot(aes(x = sex, y = life_satisfaction)) +
    geom_boxplot() +
    theme_minimal()
```

## Conduct the independent samples T-test

Remember, the hypotheses are:

::: columns
::: {.column width="50%"}
$H_0$: There is **no** significant difference in the mean life satisfaction between male and female participant.
:::

::: {.column width="50%"}
$H_1$: There is a significant difference in the mean life satisfaction between male and female participant.
:::
:::

```{r}
#| echo: true
#| output-location: column

t.test(life_satisfaction ~ sex, 
       data = wvs_cleaned, 
       var.equal = FALSE)
```
::: {.callout-tip appearance="simple"}

### Notice that we are using Welch's t-test instead of Students' t-test

Welch's t-test (also known as **unequal variances t-test**, is a more robust alternative to Student's t-test. It is often used when two samples have unequal variances and possibly unequal sample sizes. 

:::


## T-Test: Paired Sample T-Test

Unfortunately, our data is not suitable for paired T-Test. For demo purposes, we are going to use a built-in sample datasets called `sleep` from the base R dataset.

The dataset is already loaded, so you can use it right away!

-   type `View(sleep)` in your R console (bottom left), and then press enter. RStudio will open up the preview of the dataset.
-   type `?sleep` in your R console to view the help page (a.k.a vignette) about this dataset.
-   type `data()` in your console to see what are the available datasets that you can use for practice!

```{r}
#| echo: true
#| output-location: slide

sleep <- as_tibble(sleep)
print(sleep)

```

## Visualise the before (group 1) and after (group 2)

```{r}
#| echo: true

sleep |> 
    group_by(group) |> 
    summarise(n = n(), mean = mean(extra), sd = sd(extra), variance = var(extra))
```


Visualization: 

```{r}
#| echo: true
#| output-location: slide

sleep |> 
    ggplot(aes(x = group, y = extra)) +
    geom_boxplot() +
    theme_minimal()
```

## Transform the data shape

The data is in long format. let's transform it into wide format so that we can conduct the analysis more easily.

```{r}
#| echo: true
#| output-location: fragment

sleep_wide <- sleep |> 
    pivot_wider(names_from = group, values_from = extra, 
                names_prefix = "group_")
print(sleep_wide)

```

## Conduct the paired-sample T-test

Remember, the hypotheses are:

::: columns
::: {.column width="50%"}
$H_0$: There is no significant difference in the increase in hours of sleep.
:::

::: {.column width="50%"}
$H_1$: There is a significant difference in the increase in hours of sleep.
:::
:::

```{r}
#| echo: true
#| output-location: fragment

t.test(Pair(sleep_wide$group_1, sleep_wide$group_2) ~ 1,
       data = sleep)

```
## Learning Check #2

Using the pre-loaded dataset called `CO2`, conduct the T-test betwen the two different `Treatment` to compare if there's any difference in the carbon dioxide `uptake` between the two treatments. Type `?CO2` in your Console to retrieve the vignette of this data. 

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show answer"

t.test(uptake ~ Treatment, data = CO2)

```
Visualize the data:

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show answer"
#| output-location: slide

CO2 |> ggplot(aes(x = Treatment, y = uptake)) +
    geom_boxplot(width = 0.2) +
    theme_minimal()

```

## ANOVA (Analysis of Variance)

ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups or samples and determine if the differences are statistically significant.

There are two 'mainstream' ANOVA that will be covered in this workshop:

::: incremental
-   **One-Way ANOVA**: comparing means across two or more independent groups (levels) of a [**single**]{.underline} independent variable.
-   **Two-Way ANOVA**: comparing means across two or more independent groups (levels) of [**two**]{.underline} independent variable.
-   Other types of ANOVA that you may encounter: Repeated measures ANOVA, Multivariate ANOVA (MANOVA), ANCOVA, etc.
:::

## One-Way ANOVA: Sample problem and result

RQ: **Is there a significant difference in life satisfaction between different country?**

Let's visualize the data first!

```{r}
#| echo: true
#| output-location: slide

wvs_cleaned |> 
    ggplot(aes(x = country, y = life_satisfaction)) +
    geom_boxplot() +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Conduct the one-way Anova test

```{r}
#| echo: true
#| output-location: fragment

satisfaction_country_anova <- aov(life_satisfaction ~ country, data = wvs_cleaned)
summary(satisfaction_country_anova)
```

::: {style="font-size: 75%; width: 75%"}
-   **F-value**: the coefficient value
-   **Pr(\>F)**: the p-value
-   **Sum Sq**: Sum of Squares
-   **Mean Sq** : Mean Squares
-   **Df**: Degrees of Freedom
:::

## Post-hoc test (only when result is significant)

If your ANOVA test indicates significant result, the next step is to figure out which category pairings are yielding the significant result.

Tukey's Honest Significant Difference (HSD) can help us figure that out! Other alternative is the `pairwise.t.test`, but let's try Tukey's for now.

```{r}
#| echo: true
#| output-location: fragment
TukeyHSD(satisfaction_country_anova)
```

## ANOVA Assumptions

1.  The Dependent variable should be a continuous variable

2.  The Independent variable should be a categorical variable

3.  The observations for Independent variable should be independent of each other

4.  The Dependent Variable distribution should be approximately normal -- even more crucial if sample size is small.

    -   You can verify this by visualizing your data in histogram, or use Shapiro-Wilk Test, among other things

5.  The variance for each combination of groups should be approximately equal -- also referred to as "homogeneity of variances" or [**homoskedasticity**]{.underline}.

    -   One way to verify this is using Levene's Test

6.  No significant outliers

## Verifying the assumption: Test for Homogeneity of variance

**Levene's Test** to test for homogeneity of variance i.e. homoskedasticity

```{r}
#| echo: true
#| output-location: column

library(car)
leveneTest(life_satisfaction ~ country, 
           data = wvs_cleaned)
```

The results indicate that the p-value is more than the significance level of 0.05, suggesting that there is NO significant difference in variance across the groups. Consequently, the assumption of homogeneity of variances is NOT violated.

## Plotting Residuals: Residual vs Fitted

When we plot the residuals[^1], we can see some outliers as well:

[^1]: Residual = difference between an observed value and the mean of all values for that group.

```{r}
#| echo: true
#| output-location: fragment

plot(satisfaction_country_anova, 1)

```

## Verifying the assumptions: Test for Normality

**Shapiro-Wilk Test** to test for normality.

```{r}
#| echo: true
#| output-location: column

library(car)
set.seed(123) # set seed for reproducibility, make sure it samples the same way everytime.
shapiro.test(sample(residuals(satisfaction_country_anova), 5000))
```

The p-value from the Shapiro-Wilk test is less than the significance level of 0.05, indicating that the data significantly deviates from normality. Therefore, the assumption of normality is NOT satisfied.

## Plotting Residuals: Q-Q Plot 

We can see this better from when we plot the residuals:

```{r}
#| echo: true
#| output-location: fragment

plot(satisfaction_country_anova, 2)

```

## When the assumptions are not met...

We can use **Kruskal-Wallis rank sum test** as an non-parametric alternative to One-Way ANOVA!

```{r}
#| echo: true 

kruskal.test(life_satisfaction ~ country, data = wvs_cleaned)
```

Other alternative: **Welch's ANOVA** for when the homoskedasticity assumption is not met.

```{r}
#| echo: true

oneway.test(life_satisfaction ~ country, data = wvs_cleaned, var.equal = FALSE)
```

## Two-Way ANOVA: Sample problem and result

RQ: **Is there a significant difference in life satisfaction across religiousity and countries?**

Let's visualize the data!

```{r}
#| echo: true
#| output-location: slide

wvs_cleaned |> 
    ggplot(aes(x = religiousity, y = life_satisfaction)) +
    geom_boxplot() +
    facet_wrap(~ country) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Conduct the Two-way ANOVA test (Additive model)

```{r}
#| echo: true
#| output-location: fragment
satisfaction_relig_country_anova <- aov(life_satisfaction ~ religiousity + country, 
                                    data = wvs_cleaned)
summary(satisfaction_relig_country_anova)
```

## Post-hoc test for Two-way ANOVA

```{r}
#| echo: true
#| output-location: fragment
TukeyHSD(satisfaction_relig_country_anova)
```
## Conduct the Two-way ANOVA test (with Interaction)

"With interaction" means we are testing whether the effect of one variable (religiosity) on the outcome (life satisfaction) depends on the level of the other variable (country), or vice versa. For the R code, we use `religiousity * country` instead of `religiousity + country`

```{r}
#| echo: true
#| output-location: fragment
satisfaction_relig_country_anova <- aov(life_satisfaction ~ religiousity * country, 
                                    data = wvs_cleaned)
summary(satisfaction_relig_country_anova)
```
## Interaction plot

To better see this effect, let's plot the interaction.

```{r}
#| echo: true
#| output-location: slide

wvs_cleaned |> 
    ggplot(aes(x = religiousity, y = life_satisfaction, 
                          group = country, color = country)) + # lines will be grouped and colored by country
    stat_summary(fun = mean, geom = "point") + # Add points to show mean life_satisfaction for each religiosity by country
    stat_summary(fun = mean, geom = "line") + # Connect the points with lines 
    theme_minimal()
    
```

## Interpreting our interaction plot

Some observations that we can make:

-   New Zealand's pattern is distinctly different from the other two countries; NZ hows highest life satisfaction for "Not a religious person" but drops sharply for "A religious person", while for Canada and Singapore, there is a similar patterns with peaks at "A religious person"
-   The interaction is most visible in how religious vs non-religious people differ across countries, with the largest difference between countries appears among non-religious people. 
-   The relationship between religiosity and life satisfaction clearly varies by country. We know this because the lines are not parallel with each other. 


## Learning Check #3

Is there a significant difference in political leaning between different age groups?

-   Visualize the data as well
-   Test for normality and homoskedasticity, and choose the appropriate test


```{r}
#| code-fold: true
#| echo: true
#| code-summary: "Show answer"

poli_age_anova <- aov(political_scale ~ age_group, data = wvs_cleaned)
summary(poli_age_anova)
```
---

TukeyHSD:

```{r}
#| code-fold: true
#| echo: true
#| code-summary: "Show answer"

TukeyHSD(poli_age_anova)
```

# Reporting result with apaTables and gtsummary

## Reporting with apaTables

`apaTables` is a package that will generate APA-formatted report table for correlation, ANOVA, and regression. It has limited customisations and few varity of tables. The documentation online is for the "development" version which is not what we will get if we install normally with `install.packages()`, so we need to rely on the vignette more. [**View the documentation here**](https://dstanley4.github.io/apaTables/index.html)


Example: get the correlation table for `political_scale`, `life_satisfaction`, and `financial_satisfaction`

```r
library(apaTables)

wvs_cleaned |> 
    select(life_satisfaction, financial_satisfaction, political_scale) |> 
    apa.cor.table( table.number = 1, filename = "fig-output/table-cor.doc")
```

## Reporting with gtsummary

Another popular packages is `gt` (stands for "great tables") and its 'add-on', `gtsummary`. It has lots of customizations (which can get overwhelming!) but fortunately the documentation is pretty good and there are plenty of code samples. [**View the documentation here**](https://www.danieldsjoberg.com/gtsummary/index.html)

Example: get the mean differences table for `political_scale`, `life_satisfaction`, and `financial_satisfaction`, grouped by `sex`

```{r}
#| echo: true
#| output-location: slide

library(gtsummary)

wvs_cleaned |> 
    dplyr::select(life_satisfaction, financial_satisfaction, political_scale, sex) |> 
    tbl_summary(by = sex) |> 
    add_difference()
```

# Recap

::: incremental

- Data distribution, normal distribution and skewed distribution. 

- Use X2  test of independence, chisq.test(), to evaluate whether there is a statistically significant relationship between two categorical variables. 

- Use a correlation test, cor.test(), to evaluate the strength and direction of a linear relationship between two variables. The coefficient is expressed in value between -1 to 1, with 0 being no correlation at all. 

- Use t-test() to compare the means of two groups of continuous data and determine if the differences are statistically significant.  

- Three types of t-tests i.e., one-sample t-test, independent samples t-test, and paired samples t-test. 

- Use ANOVA (Analysis of Variance) to compare the means of three or more groups or samples and determine if the differences are statistically significant.  

- Report results with apaTables and gtsummary. 

:::

# End of Session 4!

Next session: Linear and Logistic Regressions!
