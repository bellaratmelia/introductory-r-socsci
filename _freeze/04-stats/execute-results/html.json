{
  "hash": "adf5d6aa94f7e425f5d44a18e274b289",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Basic Inferential Stats in R: Correlation, T-Tests, and ANOVA\"\nauthor: \"Bella Ratmelia\"\nformat: revealjs\n---\n\n\n\n## Today's Outline\n\n1.  Refreshers on data distribution and research variables\n2.  Statistical tests: chi-square, t-test, correlations.\n3.  ANOVA\n\n## Refresher: Data Distribution\n\nThe choice of appropriate statistical tests and methods often depends on the distribution of the data. Understanding the distribution helps in selecting the right and validity of the tests.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Refresher: Research Variables\n\n::: columns\n::: {.column width=\"50%\"}\n[**Dependent Variable (DV)**]{.underline}\n\nThe variables that will be affected as a result of manipulation/changes in the IVs\n\n-   Other names for it: Outcome, Response, Output, etc.\n-   Often denoted as $y$\n:::\n\n::: {.column width=\"50%\"}\n[**Independent Variable (IV)**]{.underline}\n\nThe variables that researchers will manipulate.\n\n-   Other names for it: Predictor, Covariate, Treatment, Regressor, Input, etc.\n-   Often denoted as $x$\n:::\n:::\n\n## Load our data for today!\n\nUse `read_csv` from `readr` package (part of `tidyverse`) to load our data into a dataframe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import tidyverse library\nlibrary(tidyverse)\n\n# read the WVS data\nwvs_data <- read_csv(\"data-output/wvs_processed.csv\")\n\n# Convert categorical variables to factors\nwvs_data <- wvs_data |> \n    mutate(across(c(\"country\", \"sex\", \"marital_status\", \n                    \"employment\", \"religiousity\"), as.factor))\n\n# peek at the data\nglimpse(wvs_data)\n```\n:::\n\n\n\n## Chi-square test of independence\n\nThe $X^2$ test of independence evaluates whether there is a statistically significant relationship between two categorical variables.\n\nThis is done by analyzing the frequency table (i.e., contingency table) formed by two categorical variables.\n\n**Example**: Is there a relationship between `religiousity` and `marital_status` in our WVS data?\n\nTypically, we can start with visualizing the data first!\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                        \n                         Divorced Living together as married Married Separated\n  A religious person          135                        154    1562        46\n  An atheist                   56                        204     431        34\n  Don't know                    1                          7      23         0\n  Not a religious person      152                        398    1261        59\n                        \n                         Single Widowed\n  A religious person        590     118\n  An atheist                338      24\n  Don't know                  9       4\n  Not a religious person    731      66\n```\n\n\n:::\n:::\n\n\n\n## Chi-Square: Sample problem and results\n\nIs there a relationship between religiosity and marital_status?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(table(wvs_data$religiousity, wvs_data$marital_status))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(wvs_data$religiousity, wvs_data$marital_status)\nX-squared = 277.36, df = 15, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"font-size: 75%; width: 75%\"}\n-   **X-squared** = the coefficient\n-   **df** = degree of freedom\n-   **p-value** = the probability of getting more extreme results than what was observed. Generally, if this value is less than the pre-determined significance level (also called alpha), the result would be considered \"statistically significant\"\n:::\n\n*What if there is a hypothesis? How would you write this in the report?*\n\n## Correlation\n\nA correlation test evaluates the strength and direction of a linear relationship between two variables. The coefficient is expressed in value between -1 to 1, with 0 being no correlation at all.\n\n::: columns\n::: {.column width=\"33%\"}\n**Pearson's** $r$ (r)\n\n-   Measure the association between two continuous numerical variables\n-   Sensitive to outliers\n-   Assumes normality and/or linearity\n-   (*most likely the one that you learned in class)*\n:::\n\n::: {.column width=\"33%\"}\n**Kendall's** $\\tau$ (tau)\n\n-   Measure the association between two variables (ordinal-ordinal or ordinal-continuous)\n-   less sensitive/more robust to outliers\n-   non-parametric, does not assume normality and/or linearity\n:::\n\n::: {.column width=\"33%\"}\n**Spearman's** $\\rho$ (rho)\n\n-   Measure the association between two variables (ordinal-ordinal or ordinal-continuous)\n-   less sensitive/more robust to outliers\n-   non-parametric, does not assume normality and/or linearity\n:::\n:::\n\nFor more info, you can refer to this reading: [Measures of Association - How to Choose? (Harry Khamis, PhD)](https://journals.sagepub.com/doi/pdf/10.1177/8756479308317006){target=\"_blank\"}\n\n## Correlation: Sample problem and result\n\nRQ: **Is there a significant correlation between life satisfaction and financial satisfaction?**\n\nAs both variables are numerical and continuous, we can use pearson correlation.\n\nLet's start with visualizing the data, which can be used to support the explanation.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n## Conduct the correlation test\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\ncor.test(wvs_data$life_satisfaction, \n         wvs_data$financial_satisfaction, \n         method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  wvs_data$life_satisfaction and wvs_data$financial_satisfaction\nt = 66.999, df = 6401, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6274032 0.6562061\nsample estimates:\n      cor \n0.6420311 \n```\n\n\n:::\n:::\n\n\n\n::: {style=\"font-size: 75%; width: 75%\"}\n-   **cor** is the **correlation coefficient** - this is the number that you want to report.\n-   **t** is the **t-test statistic**\n-   **df** is the degrees of freedom\n-   **p-value** is the significance level of the **t-test**\n-   **conf.int** is the **confidence interval** of the coefficient at 95%\n-   **sample estimates** is the correlation coefficient\n:::\n\n## Let's try this correlation exercise! (5 mins)\n\n-   Calculate the correlation coefficient between life satisfaction and freedom\n\n    -   Which method should you use for this?\n\n    -   How strong is the correlation? i.e. would you say it's a strong correlation?\n\n    -   In which direction is the correlation?\n\n    -   Is the correlation coefficient statistically significant?\n\n    -   Visualize the relationship!\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## T-Tests\n\nA **t-test** is a statistical test used to compare the means of two groups/samples of continuous data type and determine if the differences are statistically significant.\n\n-   The Student's *t*-test is widely used when the sample size is reasonably small (less than approximately 30) **or** when the population standard deviation is unknown.\n\n## 3 types of t-test\n\n::: columns\n::: {.column width=\"30%\"}\n**One-sample T-test**\n\nTest if a specific sample mean (X̄) is statistically different from a known or hypothesized population mean (μ or mu)\n:::\n\n::: {.column width=\"40%\"}\n**Two-samples / Independent Samples T-test**\n\nUsed to compare the means of two independent groups (such as between-subjects research) to determine if they are significantly different.\n\nExamples: Men vs Women group, Placebo vs Actual drugs.\n:::\n\n::: {.column width=\"30%\"}\n**Paired Samples T-Test**\n\nUsed to compare the means of two related groups, such as repeated measurements on the same subjects (within-subjects research).\n\nExamples: Before workshop vs After workshop.\n:::\n:::\n\n## T-test: One-sample T-Test\n\nRQ: **Is the average life satisfaction in our sample significantly different from the global average of 6.5?**\n\nLet's start with visualizing the data\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nwvs_data |> \n    ggplot(aes(y = life_satisfaction, x = 1)) +\n    geom_boxplot(width = 0.2) + \n    geom_hline(yintercept = 6.5, color=\"red\") +\n    geom_label(label = \"global mean\", \n               x = 1, y = 6.6, \n               label.size = 0.15) +\n    coord_flip() +\n    theme(aspect.ratio = 1/3) \n```\n\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n## Conduct the One-sample T-Test\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nglobal_mean_satisfaction = 6.5  \n\nt.test(wvs_data$life_satisfaction,          \n       alternative = \"two.sided\", \n       mu = global_mean_satisfaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  wvs_data$life_satisfaction\nt = 26.776, df = 6402, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 6.5\n95 percent confidence interval:\n 7.060083 7.148570\nsample estimates:\nmean of x \n 7.104326 \n```\n\n\n:::\n:::\n\n\n\n*What if there is a hypothesis? How would you write this in the report?*\n\n## T-Test: Independent Samples T-Test\n\nRQ: **Is there a significant difference in life satisfaction between males and females?**\n\nLet's first take only the necessary columns and get some summary statistics, particularly on the number of samples for each group, as well as the mean, standard deviation, and variance.\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nsatisfaction_by_gender <- wvs_data |> \n    select(life_satisfaction, sex)\n\nsatisfaction_by_gender |> \n    group_by(sex) |> \n    summarise(total = n(), \n              mean = mean(life_satisfaction),\n              variance = var(life_satisfaction),\n              stdeviation = sd(life_satisfaction))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  sex    total  mean variance stdeviation\n  <fct>  <int> <dbl>    <dbl>       <dbl>\n1 Female  3232  7.11     3.18        1.78\n2 Male    3171  7.09     3.35        1.83\n```\n\n\n:::\n:::\n\n\n\n## Visualize the differences between two samples\n\nThe variance will be easier to see when we visualize it as well. As we can see, the variance for Y group is wider than the N group. This suggests that the variance might be heterogeneous (heteroskedastic).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n## Conduct the independent samples T-test\n\nRemember, the hypotheses are:\n\n::: columns\n::: {.column width=\"50%\"}\n$H_0$: There is no significant difference in the mean age between those who intend to vote \"Yes\" and those who intend to vote \"No\".\n:::\n\n::: {.column width=\"50%\"}\n$H_1$: There is a significant difference in the mean age between those who intend to vote \"Yes\" and those who intend to vote \"No\".\n:::\n:::\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nt.test(life_satisfaction ~ sex, \n       data = satisfaction_by_gender, \n       var.equal = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  life_satisfaction by sex\nt = 0.41256, df = 6387.7, p-value = 0.6799\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.06988992  0.10714841\nsample estimates:\nmean in group Female   mean in group Male \n            7.113552             7.094923 \n```\n\n\n:::\n:::\n\n\n::: {.callout-tip appearance=\"simple\"}\n\n### Notice that we are using Welch's t-test instead of Students' t-test\n\nWelch's t-test (also known as **unequal variances t-test**, is a more robust alternative to Student's t-test. It is often used when two samples have unequal variances and possibly unequal sample sizes. \n\n:::\n\n\n## T-Test: Paired Sample T-Test\n\nUnfortunately, our data is not suitable for paired T-Test. For demo purposes, we are going to use a built-in sample datasets called `sleep` from the base R dataset.\n\nThe dataset is already loaded, so you can use it right away!\n\n-   type `View(sleep)` in your R console (bottom left), and then press enter. RStudio will open up the preview of the dataset.\n-   type `?sleep` in your R console to view the help page (a.k.a vignette) about this dataset.\n-   type `data()` in your console to see what are the available datasets that you can use for practice!\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 20\nColumns: 3\n$ extra <dbl> 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0, 1.9, 0.8, …\n$ group <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ ID    <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n```\n\n\n:::\n:::\n\n\n\n## Visualise the before (group 1) and after (group 2)\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  group     n  mean    sd variance\n  <fct> <int> <dbl> <dbl>    <dbl>\n1 1        10  0.75  1.79     3.20\n2 2        10  2.33  2.00     4.01\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## Transform the data shape\n\nThe data is in long format. let's transform it into wide format so that we can conduct the analysis more easily.\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nsleep_wide <- sleep |> \n    pivot_wider(names_from = group, values_from = extra, \n                names_prefix = \"group_\")\nglimpse(sleep_wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 3\n$ ID      <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ group_1 <dbl> 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0\n$ group_2 <dbl> 1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4\n```\n\n\n:::\n:::\n\n\n\n## Conduct the paired-sample T-test\n\nRemember, the hypotheses are:\n\n::: columns\n::: {.column width=\"50%\"}\n$H_0$: There is no significant difference in the increase in hours of sleep.\n:::\n\n::: {.column width=\"50%\"}\n$H_1$: There is a significant difference in the increase in hours of sleep.\n:::\n:::\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nt.test(Pair(sleep_wide$group_1, sleep_wide$group_2) ~ 1,\n       data = sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  Pair(sleep_wide$group_1, sleep_wide$group_2)\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n```\n\n\n:::\n:::\n\n\n\n## ANOVA (Analysis of Variance)\n\nANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups or samples and determine if the differences are statistically significant.\n\nThere are two 'mainstream' ANOVA that will be covered in this workshop:\n\n::: incremental\n-   **One-Way ANOVA**: comparing means across two or more independent groups (levels) of a [**single**]{.underline} independent variable.\n-   **Two-Way ANOVA**: comparing means across two or more independent groups (levels) of [**two**]{.underline} independent variable.\n-   Other types of ANOVA that you may encounter: Repeated measures ANOVA, Multivariate ANOVA (MANOVA), ANCOVA, etc.\n:::\n\n## One-Way ANOVA: Sample problem and result\n\nRQ: **Is there a significant difference in life satisfaction between different religiousity?**\n\nLet's visualize the data first!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n## Conduct the one-way Anova test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisfaction_religiousity_anova <- aov(life_satisfaction ~ religiousity, data = wvs_data)\nsummary(satisfaction_religiousity_anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nreligiousity    3    128   42.82    13.2 1.36e-08 ***\nResiduals    6399  20752    3.24                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {style=\"font-size: 75%; width: 75%\"}\n-   **F-value**: the coefficient value\n-   **Pr(\\>F)**: the p-value\n-   **Sum Sq**: Sum of Squares\n-   **Mean Sq** : Mean Squares\n-   **Df**: Degrees of Freedom\n:::\n\n## Post-hoc test (only when result is significant)\n\nIf your ANOVA test indicates significant result, the next step is to figure out which category pairings are yielding the significant result.\n\nTukey's Honest Significant Difference (HSD) can help us figure that out! Other alternative is the `pairwise.t.test`, but let's try Tukey's for now.\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nTukeyHSD(satisfaction_religiousity_anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = life_satisfaction ~ religiousity, data = wvs_data)\n\n$religiousity\n                                                diff         lwr         upr\nAn atheist-A religious person             -0.3673683 -0.53446513 -0.20027152\nDon't know-A religious person              0.1029838 -0.60052046  0.80648800\nNot a religious person-A religious person -0.2272818 -0.35475772 -0.09980581\nDon't know-An atheist                      0.4703521 -0.24126460  1.18196879\nNot a religious person-An atheist          0.1400866 -0.02643742  0.30661054\nNot a religious person-Don't know         -0.3302655 -1.03363393  0.37310286\n                                              p adj\nAn atheist-A religious person             0.0000001\nDon't know-A religious person             0.9818620\nNot a religious person-A religious person 0.0000279\nDon't know-An atheist                     0.3245019\nNot a religious person-An atheist         0.1341519\nNot a religious person-Don't know         0.6226518\n```\n\n\n:::\n:::\n\n\n\n## ANOVA Assumptions\n\n1.  The Dependent variable should be a continuous variable\n\n2.  The Independent variable should be a categorical variable\n\n3.  The observations for Independent variable should be independent of each other\n\n4.  The Dependent Variable distribution should be approximately normal -- even more crucial if sample size is small.\n\n    -   You can verify this by visualizing your data in histogram, or use Shapiro-Wilk Test, among other things\n\n5.  The variance for each combination of groups should be approximately equal -- also referred to as \"homogeneity of variances\" or [**homoskedasticity**]{.underline}.\n\n    -   One way to verify this is using Levene's Test\n\n6.  No significant outliers\n\n## Verifying the assumption: Test for Homogeneity of variance\n\n**Levene's Test** to test for homogeneity of variance i.e. homoskedasticity\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nlibrary(car)\nleveneTest(life_satisfaction ~ religiousity, \n           data = wvs_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(>F)\ngroup    3  0.6941 0.5555\n      6399               \n```\n\n\n:::\n:::\n\n\n\nThe results indicate that the p-value is less than the significance level of 0.05, suggesting a significant difference in variance across the groups. Consequently, the assumption of homogeneity of variances is violated.\n\n## Plotting Residuals: Residual vs Fitted\n\nWhen we plot the residuals[^1], we can see some outliers as well:\n\n[^1]: Residual = difference between an observed value and the mean of all values for that group.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n## Verifying the assumptions: Test for Normality\n\n**Shapiro-Wilk Test** to test for normality.\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nlibrary(car)\nshapiro.test(wvs_data$age[0:5000])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  wvs_data$age[0:5000]\nW = 0.9726, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nThe p-value from the Shapiro-Wilk test is less than the significance level of 0.05, indicating that the data significantly deviates from normality. Therefore, the assumption of normality is not satisfied.\n\n## Plotting Residuals: Q-Q Plot \n\nWe can see this better from when we plot the residuals:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\n## When the assumptions are not met...\n\nWe can use **Kruskal-Wallis rank sum test** as an non-parametric alternative to One-Way ANOVA!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(life_satisfaction ~ religiousity, data = wvs_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  life_satisfaction by religiousity\nKruskal-Wallis chi-squared = 41.101, df = 3, p-value = 6.224e-09\n```\n\n\n:::\n:::\n\n\n\nOther alternative: **Welch's ANOVA** for when the homoskedasticity assumption is not met.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(life_satisfaction ~ religiousity, data = wvs_data, var.equal = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  life_satisfaction and religiousity\nF = 12.736, num df = 3.00, denom df = 209.51, p-value = 1.116e-07\n```\n\n\n:::\n:::\n\n\n\n## Two-Way ANOVA: Sample problem and result\n\nRQ: **Is there a significant difference in life satisfaction across religiousity and countries?**\n\nLet's visualize the data!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-stats_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n## Conduct the Two-way ANOVA test\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nsatisfaction_relig_country_anova <- aov(life_satisfaction ~ religiousity + country, \n                                    data = wvs_data)\nsummary(satisfaction_relig_country_anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nreligiousity    3    128   42.82   13.32 1.15e-08 ***\ncountry         2    192   95.83   29.82 1.29e-13 ***\nResiduals    6397  20560    3.21                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n## Post-hoc test for Two-way ANOVA\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nTukeyHSD(satisfaction_relig_country_anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = life_satisfaction ~ religiousity + country, data = wvs_data)\n\n$religiousity\n                                                diff         lwr        upr\nAn atheist-A religious person             -0.3673683 -0.53371770 -0.2010189\nDon't know-A religious person              0.1029838 -0.59737368  0.8033412\nNot a religious person-A religious person -0.2272818 -0.35418751 -0.1003760\nDon't know-An atheist                      0.4703521 -0.23808153  1.1787857\nNot a religious person-An atheist          0.1400866 -0.02569256  0.3058657\nNot a religious person-Don't know         -0.3302655 -1.03048776  0.3699567\n                                              p adj\nAn atheist-A religious person             0.0000001\nDon't know-A religious person             0.9816251\nNot a religious person-A religious person 0.0000253\nDon't know-An atheist                     0.3204859\nNot a religious person-An atheist         0.1313305\nNot a religious person-Don't know         0.6191993\n\n$country\n               diff        lwr         upr     p adj\nNZL-CAN  0.53301723  0.3565021  0.70953237 0.0000000\nSGP-CAN -0.04499362 -0.1659695  0.07598224 0.6580925\nSGP-NZL -0.57801085 -0.7703672 -0.38565452 0.0000000\n```\n\n\n:::\n:::\n\n\n\n## Let's try this ANOVA exercise! (5 mins)\n\nIs there a significant difference in political leaning between different sex?\n\n-   Visualize the data as well\n-   Test for normality and homoskedasticity, and choose the appropriate test\n\n\n\n::: {.cell}\n\n:::\n\n\n\n# End of Session 4!\n\nNext session: Linear and Logistic Regressions!\n",
    "supporting": [
      "04-stats_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}