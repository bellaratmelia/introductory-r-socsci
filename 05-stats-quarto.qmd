---
title: 'Regression and Presenting Your Result'
author: 'Bella Ratmelia'
format: revealjs
---

## Today's Outline

1.  The various packages for tables
2.  Simple Linear Regression in R
    -   Continuous predictor (one and multiple)
    -   Categorical predictor
3.  Binary Logistic Regression in R
    -   Continuous predictor
    -   Categorical predictor
4.  (if time permits) Coding your analysis and writing your report in one place with Quarto.

Note: for this session, we'll prioritize *what* the code does and *how to intepret* the output, so there will be less live coding than usual. 

## Open your project - the (possibly) easier way

1.  Go to the folder where you put your project for this workshop

2.  Find a file with `.Rproj` extension - this is the R project file that holds all the information about your project.

3.  Double click on the file. Rstudio should launch with your project loaded! This should be easier to ensure that you are loading the correct project when opening Rstudio.

## Install and load extra packages we need today

1. Type the following line in your R console (bottom left section). Type them one by one.
    -   `install.packages("apaTables")`
    -   `install.packages("huxtable")`
    -   `install.packages("gtsummary")`
    -   `install.packages("car")`

2. Create a new R script called `session-5.R`

3. Paste the following lines into the script:

```{r}
#| echo: true

# loading the packages
library(apaTables)
library(huxtable)
library(gtsummary)
library(car)
library(tidyverse)

```


## Load our data for today!

```{r}
#| echo: true
#| label: load-data
#| message: false
#| output: false

# read the CSV with WVS data
wvs_cleaned <- read_csv("data-output/wvs_cleaned_v1.csv")

# Convert categorical variables to factors
columns_to_convert <- c("country", "religiousity", "sex", "marital_status", "employment", "age_group")

wvs_cleaned <- wvs_cleaned |> 
    mutate(across(all_of(columns_to_convert), as_factor))

# peek at the data, pay attention to the data types!
glimpse(wvs_cleaned)
```

# The various packages for tables

## apaTables

`apaTables` is a package that will generate APA-formatted report table for correlation, ANOVA, and regression. It has limited customisations and few varity of tables. The documentation online is for the "development" version which is not what we will get if we install normally with `install.packages()`, so we need to rely on the vignette more. [**View the documentation here**](https://dstanley4.github.io/apaTables/index.html)


Example: get the correlation table for `political_scale`, `life_satisfaction`, and `financial_satisfaction`

```r
library(apaTables)

wvs_cleaned |> 
    select(life_satisfaction, financial_satisfaction, political_scale) |> 
    apa.cor.table( table.number = 1, filename = "fig-output/table-cor.doc")
```

This code will create a word document with the tables already formatted in APA style inside.

## gtsummary

Another popular packages is `gt` (stands for "great tables") and its 'add-on', `gtsummary`. It has lots of customizations (which can get overwhelming!) but fortunately the documentation is pretty good and there are plenty of code samples. [**View the documentation here**](https://www.danieldsjoberg.com/gtsummary/index.html)

Example: get the mean differences table for `political_scale`, `life_satisfaction`, and `financial_satisfaction`, grouped by `sex`

```{r}
#| echo: true
#| output-location: slide

library(gtsummary)

wvs_cleaned |> 
    dplyr::select(life_satisfaction, financial_satisfaction, political_scale, sex) |> 
    tbl_summary(by = sex) |> 
    add_difference()
```

## How to save gtsummary() tables

1.  The tables will be displayed under the "Viewer" tab on the lower right hand side of Rstudio.
2.  Select the entire tables, and then copy it with Ctrl + C (or Cmd + C on Macbook)
3.  Paste the table with Ctrl + V (or Cmd + V on Macbook) to your word document or Google doc.

# Simple Linear Regression 

## Simple Linear Regression: what is it?

**Linear regression** is a statistical method used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors) by fitting a linear equation to the observed data. The math formula looks like this:

::: columns
::: {.column width="30%"}
$$
Y = \beta_0 + \beta_1X + \varepsilon
$$
:::

::: {.column style="font-size: 75%; width: 70%;"}
-   $Y$ - the dependent variable; **must be continuous**
-   $X$ - the independent variable (if there are more than one, there will be $X_1$ , $X_2$ , and so on. This can be ordinal, nominal, or continuous
-   $\beta_0$ - the y-intercept. Represents the expected value of independent variable $Y$ when independent variable(s) $X$ are set to zero.
-   $\beta_1$ - the slope / coefficient for independent variable
-   $\varepsilon$ - the error term. (In some examples you might see this omitted from the formula).
:::
:::

Examples:

-   Does a person's **age** affect their **life satisfaction**?
-   Do a person's **age** and **country** affect their **life satisfaction**?

Remember, regression allows us to see how one variable changes in response to another.

## Correlation vs Regression

| Features | Correlation | Regression |
|------|------|------|
| Purpose    | Describe strength & direction of association i.e. how two variables move together   | Predict, model relationships, explain impact    |
| Variables    | Two quantitative (symmetrical)    | One dependent (Y), one or more independent (X)    |
| Causality  | Does NOT imply causation    | Can *suggest* causation (with proper and robust research design)    |
| Output | correlation coefficient    | Coefficients (slopes), R-squared, predicted values    |
| Question  | Are X and Y related? How strongly?    | How does X affect Y? Can we predict Y from X?    |



## Linear Regression: One continuous predictor

**Research Question:** Does a person's **age** influence their **life satisfaction**?

- The outcome/DV ($Y$): `life_satisfaction`
- The predictor/IV ($X$): `age`

```{r}
#| echo: true
#| output-location: fragment
life_model1 <- lm(life_satisfaction ~ age, data = wvs_cleaned)
summary(life_model1) #summarize the result
```

--- 

::: {style="font-size: 80%"}

-   **Call**: the formula

-   **Residuals**: overview on the distribution of residuals (expected value minus observed value) -- we can plot this to check for homoscedasticity

-   **Coefficients**: shows the intercept, the regression coefficients for the predictor variables, and their statistical significance

-   **Residual standard error**: the average difference between observed and expected outcome by the model. Generally the lower, the better.

-   **R-squared & Adjusted R-squared**: indicates the proportion of variation in the outcome that can be explained by the model (i.e. goodness of fit).

-   **F-statistics**: indicates whether the model as a whole is statistically significant and whether it explains more variance than just the baseline (intercept-only) model.
:::

## Narrating the results

Here is one possible way to narrate your result:

> A linear regression analysis was conducted to assess the influence of age on life satisfaction in Canada, Singapore, and New Zealand. Age is a statistically significant but weak predictor (B = 0.016, SE = 0.001, p < 0.001), indicating that for each additional year of age, the life satisfaction is increased by 0.016 units. 

> The model was statistically significant (F(1, 6401) = 147.7, p < 0.001) and explained approximately 2% of the variance in life satisfaction (RÂ² = 0.022, Adjusted RÂ² = 0.0224). 

## Present your regression result - huxtable

```{r}
#| echo: true

huxreg("life satisfaction" = life_model1)

```

## Linear Regression: Multiple continuous predictors

**Research Question:** Do a person's age and financial satisfaction their life satisfaction?

- The outcome/DV ($Y$): `life_satisfaction`
- The predictors/IV ($X$): `financial_satisfaction` and `age`

```{r}
#| echo: true
#| output-location: slide

life_model2 <- lm(life_satisfaction ~ financial_satisfaction + age, 
                  data = wvs_cleaned)
summary(life_model2)
```
## Possible way to explain the result

> A multiple regression analysis was conducted to examine how financial satisfaction and age predict life satisfaction. Financial satisfaction was a strong predictor (B = 0.529, SE = 0.008, p < 0.001), indicating that for each one-unit increase in financial satisfaction, life satisfaction increased by 0.529 units. Age was also a significant but weaker predictor (B = 0.006, SE = 0.001, p < 0.001), with each additional year of age associated with a 0.006-unit increase in life satisfaction.

> The model was statistically significant (F(2, 6400) = 2268, p < 0.001) and explained 41.5% of the variance in life satisfaction (RÂ² = 0.415, Adjusted RÂ² = 0.415). 

## Present your regression result - tbl_regression() from gtsummary

```{r}
#| echo: true

life_model2 |> tbl_regression() |> bold_p()
```

::: aside
More on `tbl_regression()` by `gtsummary`: <https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html>
:::

## Reporting result: A sample regression table

:::: {.columns}

::: {.column width="60%"}
![](images/regression-table.jpg)
:::

::: {.column width="40%"}
You might encounter different table formats when reporting regression results, but there are some key elements that should generally be included. 

These are: the number of observations ($N$), the coefficients (B = unstandardized, raw coeff in original unit of measurements; $\beta$ = standardized, converted to standard deviation units), standard errors (SE), confidence intervals (95% CI), and p-values. Other metrics to include are the $R^2$ and $F$ statistics. 
:::

::::


::: aside

- The above screenshot is taken from <https://www.ncfr.org/system/files/2017-01/regression_0.pdf>
- [APA recommended format](https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables#regression)

:::

## Presenting your models {.scrollable}

If you get an error saying "huxreg not found", you may need to:

1.  Install the library by running this line in your terminal: `install.packages("huxtable")`

2.  And then load them to your script with this line: `library(huxtable)`

For more info about huxreg, go here: <https://cran.r-project.org/web/packages/huxtable/vignettes/huxreg.html>

```{r}
#| echo: true
#| output-location: slide

huxreg("life_satisfaction (model1)" = life_model1, 
       "life_satisfaction (model2)" = life_model2,
       number_format = 4,
       bold_signif = 0.05,
       statistics = c( "R squared" = "r.squared", "N" = "nobs", "F" = "statistic",
      "P value" = "p.value"))
```



## FYI: Multicollinearity

Caution! When doing regression-type of tests, watch out for multicollinearity.

Multicollinearity is a situation in which two or more predictor variables are highly correlated with each other. This makes it difficult to determine the specific contribution of each predictor variable to the relationship.

One way to check for it:

-   Assess the correlation between your predictor variables in your model using Variance Inflation Factor (VIF)

-   If they seem to be highly correlated (\> 5 or so), one of the easiest (and somewhat acceptable) way is to simply remove the less significant predictor from your model :D

```{r}
#| echo: true
car::vif(life_model2)
```

## Linear Regression: One categorical predictor

**Research Question**: Explore the difference in life satisfaction between age groups

- The outcome/DV ($Y$): `life_satisfaction`
- The predictor/IV ($X$): `age_group`

::: callout-note
Before proceeding with analysis, ensure that all the categorical variables involved are cast as factors!
:::

```{r}
#| echo: true

str(wvs_cleaned$life_satisfaction)
str(wvs_cleaned$age_group)
```

## Continuing the analysis

The analysis summary should look like this:

```{r}
#| echo: true
life_model3 <- lm(life_satisfaction ~ age_group, data = wvs_cleaned)
summary(life_model3)
```

-   When interpreting a categorical predictor in regression, one category is treated as the reference category, which serves as the baseline for comparison. In this case, the reference category corresponds to the intercept.

-   By default, the first category in the data is used as the reference category, unless specified.

## Narrating the result

Here is one possible way to narrate your result:

> A linear regression analysis was conducted to examine how age groups predict life satisfaction. With the youngest age group (15-28) as the reference group (intercept = 6.66, SE = 0.06), all other age groups showed significantly higher life satisfaction. The 29-44 age group scored 0.37 units higher (SE = 0.07, p < 0.001), the 45-60 age group scored 0.35 units higher (SE = 0.07, p < 0.001), and the 61+ age group showed the largest difference, scoring 0.87 units higher (SE = 0.07, p < 0.001) than the reference group.

> The model was statistically significant (F(3, 6399) = 55.44, p < 0.001) but only explained 2.5% of the variance in life satisfaction (RÂ² = 0.025, Adjusted RÂ² = 0.025).

## Presenting the regression table {.scrollable}

```{r}
#| echo: true
#| output-location: slide

huxreg("life satisfaction (model3)" = life_model3,
       number_format = 4,
       bold_signif = 0.05,
       statistics = c( "R squared" = "r.squared", "N" = "nobs", "F" = "statistic",
      "P value" = "p.value"))
```

## Categorical predictor: changing the reference

Let's change the reference category for `age_group` variable to "61+".

```{r}
#| echo: true

wvs_cleaned <- wvs_cleaned |> 
    mutate(age_group = relevel(age_group, ref = "61+"))

str(wvs_cleaned$age_group)
```

Re-run the analysis with the new reference category

```{r}
#| echo: true
#| output-location: slide
life_model3a<- lm(life_satisfaction ~ age_group, data = wvs_cleaned)
summary(life_model3a)
```


## Changing baseline category for ordered factor

Assuming we have a column with ordered factor in our data called `education` with the level of `Primary`, `Secondary`, and `Tertiary`. This is how we can change the order, if we want to put `Secondary` before `Primary`.

``` r
wvs_cleaned <- wvs_cleaned |> 
    mutate(education = factor(education, 
                         levels = c("Secondary", "Primary", "Tertiary"), 
                         ordered = TRUE))
```

::: {.callout-note appearance="simple" title="Releveling ordered factor"}
Previously, we could use the `relevel()` function to change the reference category for ordered factors. However, in recent R versions, this no longer works for ordered factors, so we now use the method shown in the code above. The `relevel()` function still works for unordered factors.
:::



# Binary Logistic Regression

## Binary Logistic Regression - what is it?

Also known as simply logistic regression, it is used to model the relationship between a set of independent variables and a binary outcome.

These independent variables can be either categorical or continuous.

Binary Logistics Regression formula:

$$
logit(P) = \beta_0 + \beta_1X_1 + \beta_2X_2 + â€¦ + \beta_nX_n
$$

It can also be written like below, in which the $logit(P)$ part is expanded:

$$
P = \frac{1}{1 + e^{-(\beta_0 + \beta_1X)}}
$$

## Binary Logistic Regression Examples

-   Does a personâ€™s **age** and **education level** influence whether they will vote Democrat or Republican in the US election?
-   Does the number of **hours spent studying** impact a studentâ€™s likelihood of passing a module? (pass/fail outcome)

In essence, the goal of binary logistic regression is to estimate the probability of a specific event happening when there are only two possible outcomes (hence the term "binary").

## Binary Logistic Regression: One Continuous Predictor

**Research Question**: Does a participant's **political alignment** affect the likelihood of being satisfied with life?

-   The outcome/DV ($Y$): `satisfied` 

    -   Our outcome is a continuous variable, but for the purpose of this workshop practice, let's define the outcome as "Satisfied" if the life_satisfaction score is more than 7 (inclusive), and "not Satisfied" if the score is less than 7. 
-   The predictor/IV ($X$): `political_scale`



## Dummy-coding dependent variable

Before we proceed with the calculations, we need to **dummy code** the dependent variable into 1 and 0, with 1 = Satisfied and 0 = Not Satisfied. [More info on dummy coding here](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-dummy-coding/)

```{r}
#| echo: true
# First, we need to create a binary outcome
wvs_cleaned <- wvs_cleaned |>
    mutate(satisfied = if_else(life_satisfaction >= 7, 1, 0))
# the if_else is from dplyr package (from session 2)

```

```{r}
wvs_cleaned |> select(life_satisfaction, satisfied) |> print(n = 5)
```

## Conduct the analysis

Let's conduct the analysis!

```{r}
#| echo: true

life_model5 <- glm(satisfied ~ political_scale,
                family = binomial, 
                data = wvs_cleaned)

summary(life_model5)
```

## Exponentiate the coefficients

If you recall the formula, the results are expressed in Logit Probability. As we typically report the result in terms of Odd Ratios (OR), perform exponentiation on the coefficients.

```{r}
#| echo: true

exp(coef(life_model5))

```
We can also use `tbl_regression()` to do this for us:

```{r}
#| echo: true
library(gtsummary)
life_model5 |> tbl_regression(exponentiate = TRUE) |> 
    bold_p() 
```


::: aside
More on `tbl_regression()` by `gtsummary`: <https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html>
:::

## Get the Ï‡Â² (chi-squared) to report model significance

Ï‡Â² (Chi-squared) goodness of fit tests whether your model fits the data significantly better than a null model (model with no predictors). As you can see, it is not in the `glm()` result, but we can calculate this somewhat manually using the Null and Residual deviance and df like so:

*   Null deviance: 7731
*   Residual deviance: 7690
*   Difference: 7731 - 7690 = 41 (this is the Ï‡Â² value)

To get the p-value, we need the degree of freedom and the Ï‡Â²:

*   df = difference in degrees of freedom (6402 - 6401 = 1)
*   Ï‡Â² = 41

```{r}
#| echo: true

pchisq(41, df=1, lower.tail=FALSE)  # will give p-value
```


## Get the Ï‡Â² (chi-squared) and Pseudo R-squared (for reporting purposes)

In the report, the model's Ï‡Â² (chi-squared) and R-squared (indicating the proportion of variance that can be explained by the model) should be included. However, since the resulting item does not have this info, we will call upon `DescTools`' `PseudoR2()` function to help!

```{r}
#| echo: true
#| output-location: column

# by default, McFadden will be used
DescTools::PseudoR2(life_model5) 
```

We can retrieve the R-squared value and the Ï‡Â² (also known as the G2 value, which stands for Goodness-of-fit) at the same time like so:

```{r}
#| echo: true
#| output-location: column

DescTools::PseudoR2(life_model5, 
                    which = c("G2", "Nagelkerke"))
```


## In case you need to get other Rsquared methods

```{r}
#| echo: true

DescTools::PseudoR2(life_model5, 
                    which = "all")
```

## Possible interpretation of the result

**Possible interpretation:**

*(The intercept isn't typically interpreted as an odds ratio, so we'll ignore that for now)* 

> A logistic regression was performed to ascertain the effects of political scale alignment on the likelihood that individuals will be satisfied with life versus not satisfied. The logistic regression model was statistically significant, Ï‡Â² (1, N = 6403) = 41.02, p < .001. 

> The model explained 0.9% (Nagelkerke RÂ²) of the variance in life satisfaction. Political scale alignment was associated with an increased likelihood of being satisfied with life (OR = 1.09, 95% CI [1.06, 1.12], p < .001), indicating that for each one-unit increase in political scale alignment (1-10), the odds of being satisfied with life increased by 9%.

::: aside
More on interpreting Odds Ratio: <https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/>
:::


## Why do I need to report these?

* Ï‡Â² (Chi-squared) goodness of fit tests whether your model fits the data significantly better than a null model (model with no predictors). 

* "Standard" R-squared isn't typically reported for logistic regression because it's not as meaningful as it is in linear regression. But we do still want to see how much of the variance in the data can be explained by the model. 

* Pseudo R-squared tries to mimic traditional R-squared by showing how much of the variation in the outcome your model explains, but it's adjusted to work with binary outcomes. It's sort of answering the question of "How well does my model explain the data?"

* It is possible to have a significant Ï‡Â² (meaning your model is statistically significant and better than nothing) but a low Pseudo R-squared (showing it still doesn't explain much variation). This isn't contradictory - it just means your model is better than random guessing but there's still a lot of unexplained variation. (Pretty common in social sciences; after all, human behaviours are complex!)


## FYI - IRL sample of reporting Logistic Regression

Below is a sample of how you may want to narrate your result. Note the resulting values mentioned in the paragraph below.

![](images/logreg-reporting.jpg)

In a nutshell, you will most likely have to mention the p-value, the coefficients (for linear regressions), the Odd Ratios (for logistic regression) with the confidence intervals, the chi-squared (Ï‡Â²), and the R-squared. You should also include these information in your regression table. 

::: aside
The screenshot is retrieved from <https://statistics.laerd.com/spss-tutorials/binomial-logistic-regression-using-spss-statistics.php>

:::


## Regression tables

Even with huxtable, you may need to edit the table further to include the missing stats 

```{r}
#| echo: true

huxreg("life_satisfaction" = life_model5, statistics = c("R2" = "r.squared", "logLik", "AIC"))
```

## Binary Logistic Regression: One Categorical Predictor

**Research Question**: Does **religiousity** affect the likelihood of being satisfied with life?

-   The outcome/DV ($Y$): `satisfied` 
-   The predictor/IV ($X$): `religiousity` - let's set "A religious person" as the reference category!

```{r}
#| echo: true
#| output-location: slide

wvs_cleaned <- wvs_cleaned |> 
    mutate(religiousity = relevel(religiousity, ref = "A religious person"))

life_model6 <- glm(satisfied ~ religiousity,
                      family = "binomial",
                      data = wvs_cleaned)

summary(life_model6)
exp(coef(life_model6))
```

## Exponentiate the coefficients

If you recall the formula, the results are expressed in Logit Probability. As we typically report the result in terms of Odd Ratios (OR), perform exponentiation on the coefficients.

```{r}
#| echo: true

library(gtsummary)
life_model6 |> tbl_regression(exponentiate = TRUE) |>  
    bold_p()
```


## Get the Rsquared

Let's retrieve the R-squared value and the Chi-square Ï‡Â² value (also known as the G2 value, which stands for Goodness-of-fit)


```{r}
#| echo: true
DescTools::PseudoR2(life_model6, 
                    which = c("G2", "Nagelkerke"))
```

```{r}
#| echo: true
# get the other R-squared if you like
DescTools::PseudoR2(life_model6, 
                    which = "all")
```

## Narrating the results

**Possible interpretation:**

*(The intercept isn't typically interpreted as an odds ratio, so we'll ignore that for now)* 

> A logistic regression was performed to ascertain the effects of religiosity on the likelihood that individuals will be satisfied with life versus not satisfied. The logistic regression model was statistically significant, Ï‡Â² (3, N = 6403) = 15.65, p < .001. 

> The model explained 0.3% (Nagelkerke RÂ²) of the variance in life satisfaction. Compared to religious persons (reference group), being non-religious was associated with lower odds of life satisfaction (OR = 0.84, 95% CI [0.74, 0.95], p = .004), and being an atheist was also associated with lower odds of life satisfaction (OR = 0.76, 95% CI [0.65, 0.89], p < .001). There was no significant difference in life satisfaction for those who were unsure about their religious beliefs compared to religious persons (OR = 1.24, 95% CI [0.61, 2.52], p = .550).

::: aside
More on logistic regression: <https://stats.oarc.ucla.edu/r/dae/logit-regression/>
:::

## Present both models in a table {.scrollable}

Note that the R-squared is missing here, so be sure to put that in when you paste this table to your document. 

```{r}
#| echo: true
library(huxtable)
huxreg(life_model5, life_model6)
```


# What is Quarto? What is Markdown?

## **Markdown (Specifically, R Markdown)** {.incremental}

-   **Markdown** is a lightweight markup language that provides a simple and readable way to write formatted text without using complex HTML or LaTeX. It is designed to make authoring content easy for everyone!

    -   Markdown files can be converted into HTML or other formats.
    -   Generic Markdown file will have `.md` extension.

-   **R Markdown** is an extension of Markdown that incorporates R code chunks and allows you to create dynamic documents that integrate text, code, and output (such as tables and plots).

    -   RMarkdown file will have `.Rmd` extension.

## RMarkdown in action

![](images/rmarkdown.png){fig-align="center"}

## How it all works

![Illustration by Allison Horst (www.allisonhorst.com)](images/rmarkdown_wizards.png){fig-align="center"}

## Quarto

-   **Quarto** is a [multi-language]{.underline}, [next-generation]{.underline} version of R Markdown from Posit and includes dozens of new features and capabilities while at the same being able to render most existing Rmd files without modification.

![Illustration by Allison Horst (www.allisonhorst.com)](images/quarto-rendering.png){fig-align="center"}

## Quarto in action: R scripts + Markdown

![](images/quarto.png){fig-align="center"}

## R Scripts vs Quarto

::: columns
::: {.column width="50%"}
**R Scripts**

-   Great for quick debugging, experiment

-   Preferred format if you are archiving your code to GitHub or data repository

-   More suitable for "production" tasks e.g. automating your data cleaning and processing, custom functions, etc.
:::

::: {.column width="50%"}
**Quarto**

-   Great for report and presentation to showcase your research insights/process as it integrates code, narrative text, visualizations, and results.

-   Very handy when you need your report in multiple format, e.g. in Word and PPT.

-   Fun fact: the course website and slides are all made in Quarto!
:::
:::

## If you are interested to learn more about this...

SMU Libraries regularly host Quarto workshops every semester from week 2 to week 6, taught by Prof Kam Tin Seong. 

Keep a lookout for these titles:

-   Making Your Research Reproducible with Quarto in RStudio
-   Creating Awesome Web Slides in Quarto with Revealjs
-   Building Website and Blog with Quarto


# Best Practices + More Resources

## R Best Practices

-   Use `<-` for assigning values to objects.

    -   Only use `=` when passing values to a function parameter.

-   [**Do not alter your raw data**]{.underline}; save your wrangled/cleaned data into a new file and keep it separate from the raw data.

-   Make use of R projects to organize your data and make it easier to send over to your collaborators.

    -   Having said that, when it comes to coding project, the best way to collaborate is using GitHub or similar platforms.

-   Whenever possible and makes sense for your project, follow the common convention when naming your objects, scripts, and functions. One guide that you can follow is [Hadley Wickham's tidyverse style guide](https://style.tidyverse.org/).

## References for APA Guidelines on Reporting statistics

Do check with your professor on how closely you should follow the guidelines, or if there is any specific format required.

-   [APA Style 7th Edition Numbers and Statistics Guide](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf)
-   [University of Washington - Reporting Results of Common Statistical Tests in APA Format](https://psych.uw.edu/storage/writing_center/stats.pdf)
-   [Illinois State University - A Short Guide to Handling Numbers and Statistics in APA Format (in 6th Edition)](https://about.illinoisstate.edu/mshesso-test2/reporting-statistics-in-apa-style/)
-   [Statistic Tables in APA recommended format](https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables#regression)

# Thank you for your participation ðŸ˜„

:::: {.columns}

::: {.column width="35%"}
![](images/wedidit.webp)
:::

::: {.column width="65%"}
All the best for your studies and academic journey! *(manifesting excellent grades for everyone who attended the workshop)*

Need help with R or Quarto? Please don't hesitate to contact me at [bellar@smu.edu.sg](mailto:bellar@smu.edu.sg)
:::

::::


## Post-workshop survey

Please scan this QR code or click on the link below to fill in the post-workshop survey. It should not take more than 3-4 minutes.

Survey link: <https://smusg.asia.qualtrics.com/jfe/form/SV_ernNP3dIB4Kyffo>

![](images/post-workshop-qrcode.png){width="500"}


# Appendix

Extra exercises to try at home

## Let's try this Linear Regression exercise! 

Create a regression model called `life_model4` that predicts the `life_satisfaction` score based on `sex`. The reference category should be 'Male'

```{r}
#| echo: true
#| output: true
#| code-fold: true

life_model4 <- lm(life_satisfaction ~ sex, data = wvs_cleaned)
summary(life_model4)
```

## Let's try this Logistic Regression exercise!

Create a regression model called `life_model7` that predicts the likelihood of being satisfied with life based on `sex`. 

```{r}
#| echo: true
#| output: false
#| code-fold: true

life_model7 <- glm(satisfied ~ sex,
                  family = "binomial", 
                  data = wvs_cleaned) 

summary(life_model7)
exp(coefficients(life_model7))
```